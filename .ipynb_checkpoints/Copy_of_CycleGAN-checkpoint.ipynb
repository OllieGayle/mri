{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CycleGAN",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VIGyIus8Vr7",
        "colab_type": "text"
      },
      "source": [
        "Take a look at the [repository](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) for more information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wNjDKdQy35h",
        "colab_type": "text"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRm-USlsHgEV",
        "colab_type": "code",
        "outputId": "cf830c43-c443-4f54-b3b9-7cedb1280388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-CycleGAN-and-pix2pix'...\n",
            "remote: Enumerating objects: 2203, done.\u001b[K\n",
            "remote: Total 2203 (delta 0), reused 0 (delta 0), pack-reused 2203\n",
            "Receiving objects: 100% (2203/2203), 8.02 MiB | 10.71 MiB/s, done.\n",
            "Resolving deltas: 100% (1429/1429), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt3igws3eiVp",
        "colab_type": "code",
        "outputId": "fe9a16e3-18ee-429b-909e-2be29f13ef46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "%cd pytorch-CycleGAN-and-pix2pix"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/pytorch-CycleGAN-and-pix2pix\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1EySlOXwwoa",
        "colab_type": "code",
        "outputId": "2870325b-48c1-4b9f-9ec1-7337c305f614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        }
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.5.0)\n",
            "Collecting dominate>=2.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/64/593e829416c951eb35c2246430d59b86f640087e29e71f32632bcde5d0f7/dominate-2.4.0-py2.py3-none-any.whl\n",
            "Collecting visdom>=0.1.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/75/e078f5a2e1df7e0d3044749089fc2823e62d029cc027ed8ae5d71fafcbdc/visdom-0.1.8.9.tar.gz (676kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 24.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.2.1->-r requirements.txt (line 2)) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.2.1->-r requirements.txt (line 2)) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.2.1->-r requirements.txt (line 2)) (6.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from visdom>=0.1.8.3->-r requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from visdom>=0.1.8.3->-r requirements.txt (line 4)) (2.21.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from visdom>=0.1.8.3->-r requirements.txt (line 4)) (4.5.3)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from visdom>=0.1.8.3->-r requirements.txt (line 4)) (17.0.0)\n",
            "Collecting jsonpatch\n",
            "  Downloading https://files.pythonhosted.org/packages/82/53/73ca86f2a680c705dcd1708be4887c559dfe9ed250486dd3ccd8821b8ccb/jsonpatch-1.25-py2.py3-none-any.whl\n",
            "Collecting torchfile\n",
            "  Downloading https://files.pythonhosted.org/packages/91/af/5b305f86f2d218091af657ddb53f984ecbd9518ca9fe8ef4103a007252c9/torchfile-0.1.0.tar.gz\n",
            "Collecting websocket-client\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 64.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->visdom>=0.1.8.3->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->visdom>=0.1.8.3->-r requirements.txt (line 4)) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->visdom>=0.1.8.3->-r requirements.txt (line 4)) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->visdom>=0.1.8.3->-r requirements.txt (line 4)) (1.24.3)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading https://files.pythonhosted.org/packages/18/b0/a80d29577c08eea401659254dfaed87f1af45272899e1812d7e01b679bc5/jsonpointer-2.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: visdom, torchfile\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.1.8.9-cp36-none-any.whl size=655250 sha256=54b4c512918c75b226f0ac6d2e09b648b6a79de93a2a78af4722d91717a185f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/19/a7/6d589ed967f4dfefd33bc166d081257bd4ed0cb618dccfd62a\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-cp36-none-any.whl size=5712 sha256=f87f5b556237cc60972f9de4ae29bf007c062de17701360ce264c9d796ab2ea0\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/c3/d6/9a1cc8f3a99a0fc1124cae20153f36af59a6e683daca0a0814\n",
            "Successfully built visdom torchfile\n",
            "Installing collected packages: dominate, jsonpointer, jsonpatch, torchfile, websocket-client, visdom\n",
            "Successfully installed dominate-2.4.0 jsonpatch-1.25 jsonpointer-2.0 torchfile-0.1.0 visdom-0.1.8.9 websocket-client-0.57.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IkWo1mNBVYC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "59163482-674f-4edf-d9b2-5ad42baf70da"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8daqlgVhw29P",
        "colab_type": "text"
      },
      "source": [
        "# Datasets\n",
        "\n",
        "Download one of the official datasets with:\n",
        "\n",
        "-   `bash ./datasets/download_cyclegan_dataset.sh [apple2orange, orange2apple, summer2winter_yosemite, winter2summer_yosemite, horse2zebra, zebra2horse, monet2photo, style_monet, style_cezanne, style_ukiyoe, style_vangogh, sat2map, map2sat, cityscapes_photo2label, cityscapes_label2photo, facades_photo2label, facades_label2photo, iphone2dslr_flower]`\n",
        "\n",
        "Or use your own dataset by creating the appropriate folders and adding in the images.\n",
        "\n",
        "-   Create a dataset folder under `/dataset` for your dataset.\n",
        "-   Create subfolders `testA`, `testB`, `trainA`, and `trainB` under your dataset's folder. Place any images you want to transform from a to b (cat2dog) in the `testA` folder, images you want to transform from b to a (dog2cat) in the `testB` folder, and do the same for the `trainA` and `trainB` folders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrdOettJxaCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!bash ./datasets/download_cyclegan_dataset.sh horse2zebra"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdUz4116xhpm",
        "colab_type": "text"
      },
      "source": [
        "# Pretrained models\n",
        "\n",
        "Download one of the official pretrained models with:\n",
        "\n",
        "-   `bash ./scripts/download_cyclegan_model.sh [apple2orange, orange2apple, summer2winter_yosemite, winter2summer_yosemite, horse2zebra, zebra2horse, monet2photo, style_monet, style_cezanne, style_ukiyoe, style_vangogh, sat2map, map2sat, cityscapes_photo2label, cityscapes_label2photo, facades_photo2label, facades_label2photo, iphone2dslr_flower]`\n",
        "\n",
        "Or add your own pretrained model to `./checkpoints/{NAME}_pretrained/latest_net_G.pt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B75UqtKhxznS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!bash ./scripts/download_cyclegan_model.sh horse2zebra"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFw1kDQBx3LN",
        "colab_type": "text"
      },
      "source": [
        "# Training\n",
        "\n",
        "-   `python train.py --dataroot ./datasets/horse2zebra --name horse2zebra --model cycle_gan`\n",
        "\n",
        "Change the `--dataroot` and `--name` to your own dataset's path and model's name. Use `--gpu_ids 0,1,..` to train on multiple GPUs and `--batch_size` to change the batch size. I've found that a batch size of 16 fits onto 4 V100s and can finish training an epoch in ~90s.\n",
        "\n",
        "Once your model has trained, copy over the last checkpoint to a format that the testing model can automatically detect:\n",
        "\n",
        "Use `cp ./checkpoints/horse2zebra/latest_net_G_A.pth ./checkpoints/horse2zebra/latest_net_G.pth` if you want to transform images from class A to class B and `cp ./checkpoints/horse2zebra/latest_net_G_B.pth ./checkpoints/horse2zebra/latest_net_G.pth` if you want to transform images from class B to class A.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MENqQR1CVBrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp \"/content/drive/My Drive/mri_data/checkpoints/MriWeightMapper/latest_net_G_B.pth\" \"/content/drive/My Drive/mri_data/checkpoints/MriWeightMapper/latest_net_G.pth\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sp7TCT2x9dB",
        "colab_type": "code",
        "outputId": "567da048-57bf-40e6-bc4d-29bddff34175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py --dataroot \"/content/drive/My Drive/mri_data/datasets\" --name MriWeightMapper --model cycle_gan --gpu_ids 0 --batch_size 6 --epoch_count 12 --n_epochs 40 --n_epochs_decay 40 --checkpoints_dir \"/content/drive/My Drive/mri_data/checkpoints\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------- Options ---------------\n",
            "               batch_size: 6                             \t[default: 1]\n",
            "                    beta1: 0.5                           \n",
            "          checkpoints_dir: /content/drive/My Drive/mri_data/checkpoints\t[default: ./checkpoints]\n",
            "           continue_train: False                         \n",
            "                crop_size: 256                           \n",
            "                 dataroot: /content/drive/My Drive/mri_data/datasets\t[default: None]\n",
            "             dataset_mode: unaligned                     \n",
            "                direction: AtoB                          \n",
            "              display_env: main                          \n",
            "             display_freq: 400                           \n",
            "               display_id: 1                             \n",
            "            display_ncols: 4                             \n",
            "             display_port: 8097                          \n",
            "           display_server: http://localhost              \n",
            "          display_winsize: 256                           \n",
            "                    epoch: latest                        \n",
            "              epoch_count: 12                            \t[default: 1]\n",
            "                 gan_mode: lsgan                         \n",
            "                  gpu_ids: 0                             \n",
            "                init_gain: 0.02                          \n",
            "                init_type: normal                        \n",
            "                 input_nc: 3                             \n",
            "                  isTrain: True                          \t[default: None]\n",
            "                 lambda_A: 10.0                          \n",
            "                 lambda_B: 10.0                          \n",
            "          lambda_identity: 0.5                           \n",
            "                load_iter: 0                             \t[default: 0]\n",
            "                load_size: 286                           \n",
            "                       lr: 0.0002                        \n",
            "           lr_decay_iters: 50                            \n",
            "                lr_policy: linear                        \n",
            "         max_dataset_size: inf                           \n",
            "                    model: cycle_gan                     \n",
            "                 n_epochs: 40                            \t[default: 100]\n",
            "           n_epochs_decay: 40                            \t[default: 100]\n",
            "               n_layers_D: 3                             \n",
            "                     name: MriWeightMapper               \t[default: experiment_name]\n",
            "                      ndf: 64                            \n",
            "                     netD: basic                         \n",
            "                     netG: resnet_9blocks                \n",
            "                      ngf: 64                            \n",
            "               no_dropout: True                          \n",
            "                  no_flip: False                         \n",
            "                  no_html: False                         \n",
            "                     norm: instance                      \n",
            "              num_threads: 4                             \n",
            "                output_nc: 3                             \n",
            "                    phase: train                         \n",
            "                pool_size: 50                            \n",
            "               preprocess: resize_and_crop               \n",
            "               print_freq: 100                           \n",
            "             save_by_iter: False                         \n",
            "          save_epoch_freq: 5                             \n",
            "         save_latest_freq: 5000                          \n",
            "           serial_batches: False                         \n",
            "                   suffix:                               \n",
            "         update_html_freq: 1000                          \n",
            "                  verbose: False                         \n",
            "----------------- End -------------------\n",
            "dataset [UnalignedDataset] was created\n",
            "The number of training images = 381\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "model [CycleGANModel] was created\n",
            "---------- Networks initialized -------------\n",
            "[Network G_A] Total number of parameters : 11.378 M\n",
            "[Network G_B] Total number of parameters : 11.378 M\n",
            "[Network D_A] Total number of parameters : 2.765 M\n",
            "[Network D_B] Total number of parameters : 2.765 M\n",
            "-----------------------------------------------\n",
            "Setting up a new session...\n",
            "create web directory /content/drive/My Drive/mri_data/checkpoints/MriWeightMapper/web...\n",
            "(epoch: 12, iters: 300, time: 0.536, data: 1.603) D_A: 0.334 G_A: 0.313 cycle_A: 1.042 idt_A: 0.329 D_B: 0.349 G_B: 0.366 cycle_B: 0.666 idt_B: 0.408 \n",
            "End of epoch 12 / 80 \t Time Taken: 197 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 13, iters: 216, time: 0.533, data: 0.003) D_A: 0.252 G_A: 0.405 cycle_A: 0.731 idt_A: 0.229 D_B: 0.761 G_B: 0.798 cycle_B: 0.529 idt_B: 0.354 \n",
            "End of epoch 13 / 80 \t Time Taken: 193 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 14, iters: 132, time: 0.534, data: 0.003) D_A: 0.294 G_A: 0.496 cycle_A: 0.721 idt_A: 0.247 D_B: 0.231 G_B: 0.357 cycle_B: 0.511 idt_B: 0.340 \n",
            "End of epoch 14 / 80 \t Time Taken: 193 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 15, iters: 48, time: 0.667, data: 0.002) D_A: 0.229 G_A: 0.350 cycle_A: 0.698 idt_A: 0.314 D_B: 0.202 G_B: 0.329 cycle_B: 0.599 idt_B: 0.303 \n",
            "(epoch: 15, iters: 348, time: 0.536, data: 0.002) D_A: 0.374 G_A: 0.527 cycle_A: 0.478 idt_A: 0.206 D_B: 0.198 G_B: 0.363 cycle_B: 0.455 idt_B: 0.214 \n",
            "saving the model at the end of epoch 15, iters 1536\n",
            "End of epoch 15 / 80 \t Time Taken: 199 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 16, iters: 264, time: 0.534, data: 0.004) D_A: 0.214 G_A: 0.317 cycle_A: 0.654 idt_A: 0.205 D_B: 0.235 G_B: 0.348 cycle_B: 0.487 idt_B: 0.348 \n",
            "End of epoch 16 / 80 \t Time Taken: 193 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 17, iters: 180, time: 0.534, data: 0.002) D_A: 0.288 G_A: 0.474 cycle_A: 1.185 idt_A: 0.264 D_B: 0.215 G_B: 0.278 cycle_B: 0.583 idt_B: 0.528 \n",
            "End of epoch 17 / 80 \t Time Taken: 193 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 18, iters: 96, time: 0.610, data: 0.002) D_A: 0.215 G_A: 0.292 cycle_A: 0.392 idt_A: 0.219 D_B: 0.247 G_B: 0.408 cycle_B: 0.456 idt_B: 0.207 \n",
            "End of epoch 18 / 80 \t Time Taken: 193 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 19, iters: 12, time: 0.533, data: 0.003) D_A: 0.300 G_A: 0.359 cycle_A: 0.484 idt_A: 0.182 D_B: 0.243 G_B: 0.323 cycle_B: 0.395 idt_B: 0.232 \n",
            "(epoch: 19, iters: 312, time: 0.533, data: 0.001) D_A: 0.225 G_A: 0.394 cycle_A: 0.460 idt_A: 0.161 D_B: 0.212 G_B: 0.289 cycle_B: 0.368 idt_B: 0.217 \n",
            "End of epoch 19 / 80 \t Time Taken: 193 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 20, iters: 228, time: 0.534, data: 0.002) D_A: 0.287 G_A: 0.323 cycle_A: 0.410 idt_A: 0.154 D_B: 0.242 G_B: 0.277 cycle_B: 0.340 idt_B: 0.220 \n",
            "saving the model at the end of epoch 20, iters 3456\n",
            "End of epoch 20 / 80 \t Time Taken: 199 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 21, iters: 144, time: 0.597, data: 0.003) D_A: 0.223 G_A: 0.299 cycle_A: 0.600 idt_A: 0.161 D_B: 0.207 G_B: 0.274 cycle_B: 0.384 idt_B: 0.297 \n",
            "End of epoch 21 / 80 \t Time Taken: 194 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 22, iters: 60, time: 0.534, data: 0.002) D_A: 0.231 G_A: 0.317 cycle_A: 0.474 idt_A: 0.169 D_B: 0.239 G_B: 0.334 cycle_B: 0.378 idt_B: 0.237 \n",
            "(epoch: 22, iters: 360, time: 0.534, data: 0.002) D_A: 0.200 G_A: 0.272 cycle_A: 0.528 idt_A: 0.182 D_B: 0.231 G_B: 0.358 cycle_B: 0.341 idt_B: 0.222 \n",
            "End of epoch 22 / 80 \t Time Taken: 193 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 23, iters: 276, time: 0.533, data: 0.003) D_A: 0.286 G_A: 0.288 cycle_A: 0.430 idt_A: 0.149 D_B: 0.356 G_B: 0.258 cycle_B: 0.304 idt_B: 0.203 \n",
            "End of epoch 23 / 80 \t Time Taken: 193 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 24, iters: 192, time: 0.621, data: 0.002) D_A: 0.224 G_A: 0.300 cycle_A: 0.468 idt_A: 0.152 D_B: 0.252 G_B: 0.342 cycle_B: 0.358 idt_B: 0.197 \n",
            "End of epoch 24 / 80 \t Time Taken: 194 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 25, iters: 108, time: 0.533, data: 0.003) D_A: 0.294 G_A: 0.333 cycle_A: 0.318 idt_A: 0.141 D_B: 0.231 G_B: 0.308 cycle_B: 0.255 idt_B: 0.142 \n",
            "saving the model at the end of epoch 25, iters 5376\n",
            "End of epoch 25 / 80 \t Time Taken: 199 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 26, iters: 24, time: 0.527, data: 0.002) D_A: 0.237 G_A: 0.298 cycle_A: 0.341 idt_A: 0.148 D_B: 0.228 G_B: 0.325 cycle_B: 0.288 idt_B: 0.146 \n",
            "(epoch: 26, iters: 324, time: 0.534, data: 0.004) D_A: 0.185 G_A: 0.289 cycle_A: 0.390 idt_A: 0.130 D_B: 0.241 G_B: 0.434 cycle_B: 0.292 idt_B: 0.183 \n",
            "End of epoch 26 / 80 \t Time Taken: 193 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 27, iters: 240, time: 0.606, data: 0.003) D_A: 0.213 G_A: 0.307 cycle_A: 0.348 idt_A: 0.128 D_B: 0.303 G_B: 0.349 cycle_B: 0.285 idt_B: 0.174 \n",
            "End of epoch 27 / 80 \t Time Taken: 193 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 28, iters: 156, time: 0.535, data: 0.003) D_A: 0.224 G_A: 0.339 cycle_A: 0.446 idt_A: 0.156 D_B: 0.242 G_B: 0.330 cycle_B: 0.316 idt_B: 0.204 \n",
            "End of epoch 28 / 80 \t Time Taken: 193 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 29, iters: 72, time: 0.534, data: 0.003) D_A: 0.226 G_A: 0.407 cycle_A: 0.306 idt_A: 0.128 D_B: 0.240 G_B: 0.258 cycle_B: 0.289 idt_B: 0.137 \n",
            "(epoch: 29, iters: 372, time: 0.537, data: 0.002) D_A: 0.222 G_A: 0.259 cycle_A: 0.333 idt_A: 0.126 D_B: 0.240 G_B: 0.401 cycle_B: 0.289 idt_B: 0.140 \n",
            "End of epoch 29 / 80 \t Time Taken: 193 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 30, iters: 288, time: 0.604, data: 0.002) D_A: 0.202 G_A: 0.298 cycle_A: 0.492 idt_A: 0.140 D_B: 0.279 G_B: 0.468 cycle_B: 0.309 idt_B: 0.146 \n",
            "saving the model at the end of epoch 30, iters 7296\n",
            "End of epoch 30 / 80 \t Time Taken: 199 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 31, iters: 204, time: 0.534, data: 0.002) D_A: 0.212 G_A: 0.251 cycle_A: 0.343 idt_A: 0.118 D_B: 0.250 G_B: 0.351 cycle_B: 0.268 idt_B: 0.133 \n",
            "End of epoch 31 / 80 \t Time Taken: 193 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 32, iters: 120, time: 0.536, data: 0.002) D_A: 0.236 G_A: 0.264 cycle_A: 0.383 idt_A: 0.125 D_B: 0.255 G_B: 0.391 cycle_B: 0.276 idt_B: 0.165 \n",
            "End of epoch 32 / 80 \t Time Taken: 193 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 33, iters: 36, time: 0.531, data: 0.002) D_A: 0.250 G_A: 0.582 cycle_A: 0.484 idt_A: 0.123 D_B: 0.294 G_B: 0.335 cycle_B: 0.259 idt_B: 0.226 \n",
            "(epoch: 33, iters: 336, time: 0.632, data: 0.001) D_A: 0.265 G_A: 0.121 cycle_A: 0.278 idt_A: 0.115 D_B: 0.218 G_B: 0.213 cycle_B: 0.262 idt_B: 0.125 \n",
            "End of epoch 33 / 80 \t Time Taken: 194 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 34, iters: 252, time: 0.533, data: 0.003) D_A: 0.417 G_A: 0.367 cycle_A: 0.256 idt_A: 0.143 D_B: 0.271 G_B: 0.437 cycle_B: 0.292 idt_B: 0.108 \n",
            "End of epoch 34 / 80 \t Time Taken: 193 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 35, iters: 168, time: 0.534, data: 0.003) D_A: 0.229 G_A: 0.239 cycle_A: 0.366 idt_A: 0.129 D_B: 0.219 G_B: 0.273 cycle_B: 0.237 idt_B: 0.131 \n",
            "saving the model at the end of epoch 35, iters 9216\n",
            "End of epoch 35 / 80 \t Time Taken: 201 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 36, iters: 84, time: 0.538, data: 0.003) D_A: 0.203 G_A: 0.374 cycle_A: 0.358 idt_A: 0.130 D_B: 0.212 G_B: 0.292 cycle_B: 0.307 idt_B: 0.139 \n",
            "(epoch: 36, iters: 384, time: 0.391, data: 0.002) D_A: 0.219 G_A: 0.493 cycle_A: 0.338 idt_A: 0.157 D_B: 0.274 G_B: 0.229 cycle_B: 0.358 idt_B: 0.140 \n",
            "End of epoch 36 / 80 \t Time Taken: 194 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 37, iters: 300, time: 0.533, data: 0.406) D_A: 0.207 G_A: 0.242 cycle_A: 0.308 idt_A: 0.139 D_B: 0.358 G_B: 0.466 cycle_B: 0.313 idt_B: 0.133 \n",
            "End of epoch 37 / 80 \t Time Taken: 193 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 38, iters: 216, time: 0.533, data: 0.003) D_A: 0.254 G_A: 0.512 cycle_A: 0.239 idt_A: 0.126 D_B: 0.232 G_B: 0.397 cycle_B: 0.286 idt_B: 0.101 \n",
            "End of epoch 38 / 80 \t Time Taken: 193 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 39, iters: 132, time: 0.534, data: 0.003) D_A: 0.202 G_A: 0.568 cycle_A: 0.355 idt_A: 0.088 D_B: 0.267 G_B: 0.401 cycle_B: 0.203 idt_B: 0.137 \n",
            "End of epoch 39 / 80 \t Time Taken: 193 sec\n",
            "learning rate = 0.0002000\n",
            "(epoch: 40, iters: 48, time: 0.634, data: 0.002) D_A: 0.222 G_A: 0.271 cycle_A: 0.346 idt_A: 0.107 D_B: 0.287 G_B: 0.475 cycle_B: 0.257 idt_B: 0.138 \n",
            "(epoch: 40, iters: 348, time: 0.534, data: 0.002) D_A: 0.216 G_A: 0.575 cycle_A: 0.328 idt_A: 0.131 D_B: 0.222 G_B: 0.300 cycle_B: 0.311 idt_B: 0.179 \n",
            "saving the model at the end of epoch 40, iters 11136\n",
            "End of epoch 40 / 80 \t Time Taken: 199 sec\n",
            "learning rate = 0.0001951\n",
            "(epoch: 41, iters: 264, time: 0.536, data: 0.003) D_A: 0.173 G_A: 0.307 cycle_A: 0.252 idt_A: 0.130 D_B: 0.210 G_B: 0.431 cycle_B: 0.304 idt_B: 0.099 \n",
            "End of epoch 41 / 80 \t Time Taken: 193 sec\n",
            "learning rate = 0.0001902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UkcaFZiyASl",
        "colab_type": "text"
      },
      "source": [
        "# Testing\n",
        "\n",
        "-   `python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout`\n",
        "\n",
        "Change the `--dataroot` and `--name` to be consistent with your trained model's configuration.\n",
        "\n",
        "> from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix:\n",
        "> The option --model test is used for generating results of CycleGAN only for one side. This option will automatically set --dataset_mode single, which only loads the images from one set. On the contrary, using --model cycle_gan requires loading and generating results in both directions, which is sometimes unnecessary. The results will be saved at ./results/. Use --results_dir {directory_path_to_save_result} to specify the results directory.\n",
        "\n",
        "> For your own experiments, you might want to specify --netG, --norm, --no_dropout to match the generator architecture of the trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCsKkEq0yGh0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 960
        },
        "outputId": "be704850-f1b5-4ffc-cf77-6c8d84f25f2c"
      },
      "source": [
        "!python test.py --direction \"BtoA\" --dataroot \"/content/drive/My Drive/mri_data/datasets/testB\" --name MriWeightMapper --model test --no_dropout --results_dir \"/content/drive/My Drive/mri_data/results\" --checkpoints_dir \"/content/drive/My Drive/mri_data/checkpoints\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------- Options ---------------\n",
            "             aspect_ratio: 1.0                           \n",
            "               batch_size: 1                             \n",
            "          checkpoints_dir: /content/drive/My Drive/mri_data/checkpoints\t[default: ./checkpoints]\n",
            "                crop_size: 256                           \n",
            "                 dataroot: /content/drive/My Drive/mri_data/datasets/testB\t[default: None]\n",
            "             dataset_mode: single                        \n",
            "                direction: BtoA                          \t[default: AtoB]\n",
            "          display_winsize: 256                           \n",
            "                    epoch: latest                        \n",
            "                     eval: False                         \n",
            "                  gpu_ids: 0                             \n",
            "                init_gain: 0.02                          \n",
            "                init_type: normal                        \n",
            "                 input_nc: 3                             \n",
            "                  isTrain: False                         \t[default: None]\n",
            "                load_iter: 0                             \t[default: 0]\n",
            "                load_size: 256                           \n",
            "         max_dataset_size: inf                           \n",
            "                    model: test                          \n",
            "             model_suffix:                               \n",
            "               n_layers_D: 3                             \n",
            "                     name: MriWeightMapper               \t[default: experiment_name]\n",
            "                      ndf: 64                            \n",
            "                     netD: basic                         \n",
            "                     netG: resnet_9blocks                \n",
            "                      ngf: 64                            \n",
            "               no_dropout: True                          \t[default: False]\n",
            "                  no_flip: False                         \n",
            "                     norm: instance                      \n",
            "                    ntest: inf                           \n",
            "                 num_test: 50                            \n",
            "              num_threads: 4                             \n",
            "                output_nc: 3                             \n",
            "                    phase: test                          \n",
            "               preprocess: resize_and_crop               \n",
            "              results_dir: /content/drive/My Drive/mri_data/results\t[default: ./results/]\n",
            "           serial_batches: False                         \n",
            "                   suffix:                               \n",
            "                  verbose: False                         \n",
            "----------------- End -------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"test.py\", line 45, in <module>\n",
            "    dataset = create_dataset(opt)  # create a dataset given opt.dataset_mode and other options\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/data/__init__.py\", line 57, in create_dataset\n",
            "    data_loader = CustomDatasetDataLoader(opt)\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/data/__init__.py\", line 73, in __init__\n",
            "    self.dataset = dataset_class(opt)\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/data/single_dataset.py\", line 19, in __init__\n",
            "    self.A_paths = sorted(make_dataset(opt.dataroot, opt.max_dataset_size))\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/data/image_folder.py\", line 26, in make_dataset\n",
            "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
            "  File \"/usr/lib/python3.6/genericpath.py\", line 42, in isdir\n",
            "    st = os.stat(s)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzSKIPUByfiN",
        "colab_type": "text"
      },
      "source": [
        "# Visualize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Mgg8raPyizq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = plt.imread('./results/horse2zebra_pretrained/test_latest/images/n02381460_1010_fake.png')\n",
        "plt.imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G3oVH9DyqLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = plt.imread('./results/horse2zebra_pretrained/test_latest/images/n02381460_1010_real.png')\n",
        "plt.imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}